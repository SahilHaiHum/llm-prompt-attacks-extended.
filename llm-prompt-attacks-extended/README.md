# LLM Prompt Attacks

This repository contains prompt injection and jailbreak experiments on large language models (LLMs) like ChatGPT. The goal is to explore how prompt-based attacks can override safety mechanisms.

## ðŸ“‚ Contents

- `attacks/dan-jailbreak.md` â€“ DAN (Do Anything Now) prompt attack.
- `attacks/ignore-instructions.md` â€“ "Ignore system prompt" override trick.
- `attacks/grandma-mode.md` â€“ Roleplay-based social engineering bypass.
- `attacks/reverse-psychology.md` â€“ Psychological tricking of the model.
- `screenshots/` â€“ Output screenshots from LLM tests.

## ðŸ“š References

- OWASP Top 10 for LLMs
- https://llm-attacks.org/
- https://promptattack.dev/
